{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Cursor\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an app on https://developer.twitter.com/en/apply-for-access to generate the tokens\n",
    "\n",
    "import tweepy\n",
    "consumer_key = \"<CHANGE_ME>\"\n",
    "consumer_secret = \"<CHANGE_ME>\"\n",
    "access_token = \"<CHANGE_ME>\"\n",
    "access_token_secret = \"<CHANGE_ME>\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_tweets(text):\n",
    "    df = pd.DataFrame()\n",
    "    print(\"Extract tweets containting text: \",text)\n",
    "    for i in tqdm(range(2)):\n",
    "        day = date.today() + timedelta(days=i*-1)        \n",
    "        tweets = api.search_tweets(text, tweet_mode=\"extended\",lang=\"en\",count=500,until = day) \n",
    "        #to do: check how many attributes can we get from tweet objects\n",
    "        data_tweets = [\n",
    "            [\n",
    "                tweet.created_at, \n",
    "                tweet.full_text, \n",
    "                tweet.retweet_count,\n",
    "                tweet.favorite_count,\n",
    "                tweet.user.screen_name,\n",
    "                tweet.user.followers_count,\n",
    "                tweet.user.favourites_count,\n",
    "                tweet.place,\n",
    "                tweet.entities.get(\"hashtag\"),\n",
    "                tweet.entities.get(\"user_mentions\"),\n",
    "                tweet.retweeted,\n",
    "                tweet.lang\n",
    "            ] \n",
    "            for tweet in tweets]\n",
    "\n",
    "        tweet_text_df = pd.DataFrame(data=data_tweets,\n",
    "                                     columns=[\n",
    "                                        \"created_at\",\n",
    "                                        \"full_text\",\n",
    "                                        \"retweet_count\",\n",
    "                                        \"likes\",\n",
    "                                        \"user_screen_name\",\n",
    "                                        \"user_follower_count\",\n",
    "                                        \"user_favourites_count\",\n",
    "                                        \"tweet_place\",\n",
    "                                        \"tweet_hashtags\",\n",
    "                                        \"tweets_user_mentions\",\n",
    "                                        \"tweet_retweeted_by_auth_user\",\n",
    "                                        \"tweet_lang\"\n",
    "                                        ]\n",
    "                                    )        \n",
    "\n",
    "        df = pd.concat([df, tweet_text_df],ignore_index=True)        \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract tweets containting text:  @StarbucksIndia OR @Starbucks OR @StarbucksUK OR @frappuccino OR @StarbucksNews OR @StarbucksCanada OR @StarbucksMY OR #StarbucksRewards OR #Starbucks OR #Starbuckscoffee OR #Starbuckscoffe OR #starbucksreserve OR #RedCupDay OR #StarbucksNews OR #StarbucksMalaysia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "df = extract_tweets(\"@StarbucksIndia OR @Starbucks OR @StarbucksUK OR @frappuccino OR @StarbucksNews OR @StarbucksCanada OR @StarbucksMY OR #StarbucksRewards OR #Starbucks OR #Starbuckscoffee OR #Starbuckscoffe OR #starbucksreserve OR #RedCupDay OR #StarbucksNews OR #StarbucksMalaysia\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"DM_starbucks2.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns of Interest\n",
    "- full_text - To extract the sentiment of the complaint\n",
    "- created_at - Extracting weekday or weekend may give better insight on nature of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at       0\n",
       "full_text        0\n",
       "retweet_count    0\n",
       "Likes            0\n",
       "Language         0\n",
       "cleaned_text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing values in each variable.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate tweets : 0\n",
      "Number of tweets after deleting duplicate tweets : 165\n"
     ]
    }
   ],
   "source": [
    "# removing duplicate tweets\n",
    "\n",
    "df.duplicated(subset='full_text', keep= 'first').sum()\n",
    "print('Number of duplicate tweets :',df.duplicated(subset='full_text', keep= 'first').sum())\n",
    "\n",
    "df = df.drop_duplicates(subset='full_text', keep= 'first')\n",
    "print('Number of tweets after deleting duplicate tweets :',df.full_text.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the language of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    163\n",
       "id      1\n",
       "so      1\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "df['Language'] = df.full_text.apply(lambda text: detect(text))\n",
    "\n",
    "# Getting the count of Language\n",
    "df['Language'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the tweets that are in English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Language']==\"en\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Pre-Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing new lines and tabs\n",
    "def remove_newlines_tabs(text):\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').\\\n",
    "    replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text\n",
    "df['cleaned_text']=df['full_text'].apply(lambda text:remove_newlines_tabs(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html tags\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def rev_html_Tag(text):\n",
    "    soup = bs(text, \"html.parser\")\n",
    "    new_text=soup.get_text(separator=\" \")\n",
    "    return new_text\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_html_Tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any whitespace\n",
    "import re\n",
    "\n",
    "def rev_whitespace(text):\n",
    "    pattern=re.compile(r'\\s+')\n",
    "    text_new=re.sub(pattern,' ',text)\n",
    "    return text_new\n",
    "\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_whitespace(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing addiotional accented characters from text\n",
    "import unidecode\n",
    "def rev_asc(text):\n",
    "    text=unidecode.unidecode(text)\n",
    "    return text\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_asc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing any links\n",
    "def rev_link(text):\n",
    "    rev_link=re.sub(r'http\\S+','',text)\n",
    "    rev_com=re.sub(r'\\[A-Za-z]*\\.com','',rev_link)\n",
    "    return rev_com\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_link(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing repeated charactor\n",
    "def rev_rep(text):\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text)\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_rep(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing puncutations\n",
    "def rev_puc(text):\n",
    "    punc='''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~'''\n",
    "    Formatted_text=re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\",' ',text)\n",
    "    Formatted_text2=re.sub(r\"[()]+\",' ',Formatted_text)\n",
    "    return Formatted_text2\n",
    "df['cleaned_text']=df['cleaned_text'].apply(lambda text:rev_puc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing contractions with their expanded \n",
    "import contractions\n",
    "df['cleaned_text'] = df.cleaned_text.apply(lambda text: contractions.fix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lower case\n",
    "df['cleaned_text'] = df.cleaned_text.apply(lambda text: text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @speaker_aman: Lulu Mall Lucknow \\nIndia's Biggest Mall\\n#lulumall #india #Lucknow #biggest #hypermarket #foodcourt #youtube #instagram #f…\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.full_text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rt speaker aman: lulu mall lucknow india's biggest mall lulumall india lucknow biggest hypermarket foodcourt youtube instagram f.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cleaned_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Visualizing the tweets\n",
    "We want to see what are the tweets mostly about\n",
    "A wordcloud helps visualize the same better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Saumaya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['a','u','p','b','w','s','go','c','an', 'the', 'to', 'for','also','wold','rt',\"'s\",'us'])\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # Tokenizing each sentence and then word\n",
    "    tokens = [word.lower() \n",
    "              for sent in nltk.sent_tokenize(text) \n",
    "              for word in nltk.word_tokenize(sent) \n",
    "              if word.lower() not in stopwords]\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # Filtering out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Saumaya Jain/nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_211824\\521711797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtokenize_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_211824\\1668886869.py\u001b[0m in \u001b[0;36mtokenize_only\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Tokenizing each sentence and then word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     tokens = [word.lower() \n\u001b[1;32m----> 7\u001b[1;33m               \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m               \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m               if word.lower() not in stopwords]\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Saumaya Jain/nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Saumaya Jain\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokenize_text= []\n",
    "for i in range(len(df)):\n",
    "    tokenize_text.extend(tokenize_only(df['cleaned_text'].iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_word_cloud(final_data, title):\n",
    "    wordcloud = WordCloud(width=1600, height=800, max_font_size=200, stopwords = stopwords,\n",
    "                          background_color='white').generate(final_data)\n",
    "    \n",
    "    # plt the image generated by WordCloud class\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title+\"\\n\", fontsize = 16)\n",
    "    plt.show()\n",
    "\n",
    "create_word_cloud(' '.join(tokenize_text),\"Optum Word Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of top 20 used words in all the tweets\n",
    "import seaborn as sns\n",
    "df_tokens = pd.DataFrame(tokenize_text).value_counts().rename_axis('tokens').reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=df_tokens.head(20), y='tokens',x='count', color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sentiment Scores using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unit func to process one doc\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_unit_func(doc0,column_name):\n",
    "    sents_list0 = sent_tokenize(doc0)\n",
    "    vs_doc0 = []\n",
    "    sent_ind = []\n",
    "    for i in range(len(sents_list0)):\n",
    "        vs_sent0 = analyzer.polarity_scores(sents_list0[i])\n",
    "        vs_doc0.append(vs_sent0)\n",
    "        sent_ind.append(i)\n",
    "        \n",
    "    # obtain output as DF    \n",
    "    doc0_df = pd.DataFrame(vs_doc0)\n",
    "    doc0_df.columns = [x+column_name for x in doc0_df.columns]\n",
    "    doc0_df.insert(0, 'sent_index', sent_ind)  # insert sent index\n",
    "    doc0_df.insert(doc0_df.shape[1], 'sentence', sents_list0)\n",
    "    return(doc0_df)\n",
    "\n",
    "# define wrapper func\n",
    "def vader_wrap_func(corpus0,column_name):\n",
    "    \n",
    "    # use ifinstance() to check & convert input to DF\n",
    "    if isinstance(corpus0, list):\n",
    "        corpus0 = pd.DataFrame({'text':corpus0})\n",
    "    \n",
    "    # define empty DF to concat unit func output to\n",
    "    vs_df = pd.DataFrame()    \n",
    "    \n",
    "    # apply unit-func to each doc & loop over all docs\n",
    "    for i1 in range(len(corpus0)):\n",
    "        doc0 = str(corpus0.iloc[i1])\n",
    "        vs_doc_df = vader_unit_func(doc0,column_name)  # applying unit-func\n",
    "        vs_doc_df.insert(0, 'doc_index', i1)  # inserting doc index\n",
    "        vs_df = pd.concat([vs_df, vs_doc_df], axis=0)\n",
    "        \n",
    "    return(vs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Sentiment scores\n",
    "df['vader_score'] = vader_wrap_func(df.cleaned_text,\"\").groupby('doc_index')['compound'].sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Isnegative'] = np.where(df['vader_score']<0,1,0)\n",
    "df['Isnegative'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = df[df['Isnegative']==1]\n",
    "df_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text= []\n",
    "for i in range(len(df_negative)):\n",
    "    tokenize_text.extend(tokenize_only(df_negative['cleaned_text'].iloc[i]))\n",
    "\n",
    "create_word_cloud(' '.join(tokenize_text),\"Optum Word Cloud for negative tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of top 20 used words in all the tweets\n",
    "df_tokens = pd.DataFrame(tokenize_text).value_counts().rename_axis('tokens').reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=df_tokens.head(20), y='tokens',x='count', color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df = 0.9,\n",
    "                                  max_features=50,\n",
    "                                  stop_words=stopwords,\n",
    "                                  tokenizer=tokenize_only,\n",
    "                                  strip_accents = 'unicode',\n",
    "                                  ngram_range=(2,2),\n",
    "                                 )\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(df_negative.cleaned_text)    \n",
    "count_tokens = count_vectorizer.get_feature_names()\n",
    "\n",
    "print(count_matrix.shape)  # Print the dimensions of the matrix\n",
    "\n",
    "df_bigrams = pd.DataFrame(data = count_matrix.toarray(),columns = count_tokens)\n",
    "df_bigrams.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ef4abc72552eb61b68746cba4d48a1120beb1f41271372de906235cffb7582f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
